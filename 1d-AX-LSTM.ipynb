{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.polynomial import Polynomial\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from utilities import get_npz_data, get_bci_iii_data, get_project_data\n",
    "\n",
    "tf.random.set_seed(333)\n",
    "logger = logging.getLogger('1d-AX-LSTM')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard\n",
    "------\n",
    "#### Used to track the gridsearch and visualize the process of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOG_PATH = f'{os.getcwd()}/LOG/1dAX_LSTM'\n",
    "\n",
    "HYBRID_DATA_PATH = f'{os.getcwd()}/Data/Hybrid'\n",
    "BCI_IV_2a_DATA_PATH = f'{os.getcwd()}/Data/BCI'\n",
    "BCI_III_IVa_DATA_PATH = f'{os.getcwd()}/Data/BCI III IVa'\n",
    "\n",
    "EEG, TARGET = get_project_data(HYBRID_DATA_PATH, [2.0, 3.0], sec=3, offset=1)\n",
    "\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.makedirs(LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "----\n",
    "#### The first preprocessing step is the normalisation. The scheme for this is the follwing. Let us consider $\\mathbf{X}_{\\mathrm{EEG}}=[\\mathbf{x}^{(1)},\\ldots, \\mathbf{x}^{(N)}]\\in \\mathbb{R}^{k \\times N}$, where $k$ are the amount of channels and $N$ the observations. Then we take a vector $\\mathbf{x}_{k}=[\\mathrm{x}^{(1)}_{k},\\ldots, \\mathrm{x}^{(N)}_{k}]$, calculate its mean $\\mu_{k}$, standard deviation $\\sigma_{k}$ and normalise it as $\\mathbf{\\bar{x}}_{k}=[\\frac{\\mathrm{x}^{(1)}_{k}-\\mu_{k}}{\\sigma_{k}},\\ldots, \\frac{\\mathrm{x}^{(N)}_{k}-\\mu_{k}}{\\sigma_{k}}]$. Repeating this for each channel, we get $\\mathbf{\\bar{X}}_{\\mathrm{EEG}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _preprocessing(data):\n",
    "    logger.info(f'Preprocessing data')\n",
    "    normalized_data = np.empty(shape=data.shape)\n",
    "    for idx, trial in enumerate(data):\n",
    "        normalized_data[idx] = StandardScaler().fit_transform(X=trial)\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "### One Dimension-Aggregate Approximation (1d-AX)\n",
    "----\n",
    "1d-AX  can calculated by taking $\\mathrm{\\bar{x}}_{k}=[\\mathrm{\\bar{x}}^{(1)}_{k},\\ldots, \\mathrm{\\bar{x}}^{(N)}_{k}]$ for each channel, and divide it into $m$ segements $\\mathbf{y}_{k}^{(i)}$ of equal length $q=\\frac{N}{m}$. Afterwards, we take a sampled version of $\\mathbf{y}_{k}^{(i)}$ and apply linear regression to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _segment_data(data, seg_length):\n",
    "    logger.info(f'Segmenting data')\n",
    "    num_trials, num_samples, num_channels = data.shape\n",
    "    assert num_samples % seg_length == 0\n",
    "    num_segments = num_samples//seg_length\n",
    "    seg_eeg_data = np.empty(shape=(num_trials, num_segments, seg_length, num_channels))\n",
    "\n",
    "    for trial_idx in range(num_trials):\n",
    "        for segment_idx in range(num_segments):\n",
    "            lower_limit = segment_idx * seg_length\n",
    "            upper_limit = lower_limit + seg_length\n",
    "            seg_eeg_data[trial_idx, segment_idx] = data[trial_idx, lower_limit:upper_limit, :]\n",
    "\n",
    "    return seg_eeg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _linear_regression(data):\n",
    "    num_trials, num_segments, num_samples, num_channels = data.shape\n",
    "    x = np.linspace(1, num_samples, num_samples)\n",
    "\n",
    "    K = np.zeros(shape=(num_trials, num_segments, num_channels))\n",
    "    A = np.zeros(shape=(num_trials, num_segments, num_channels))\n",
    "    for trial_idx, trial in enumerate(data):\n",
    "        for segment_idx, segment in enumerate(trial):\n",
    "            for channel_idx in range(num_channels):\n",
    "                (c1, c0) = np.polyfit(x, segment[:, channel_idx], deg=1)\n",
    "                t_mean = np.mean(segment[:, channel_idx])\n",
    "                K[trial_idx, segment_idx, channel_idx] = c1\n",
    "                A[trial_idx, segment_idx, channel_idx] = c1 * t_mean + c0\n",
    "\n",
    "    return K, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel Weighting\n",
    "----\n",
    "#### Following the 1d-AX step, we calculate spatial filters for dimensionality reduction. The idea is to gain filters $\\mathbf{W_{\\mathbf{K}}}$ and $\\mathbf{W_{\\mathbf{A}}}$, which reduce the dimensionality of $\\mathbf{K}$ and $\\mathbf{A}$ by taking $\\mathbf{K}'=\\mathbf{W_{\\mathbf{K}}}\\mathbf{K}$, as well as $\\mathbf{A}'=\\mathbf{W_{\\mathbf{A}}}\\mathbf{A}$.\n",
    "\n",
    "# LSTM and Softmax Regression\n",
    "----\n",
    "#### Subsequently, the output of the filters $\\mathbf{K}'$, $\\mathbf{A}'$ is fed into two LSTMs in parallel. Output of the LSTM is merged and fed into a Softmax Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _fit(data, num_reduced_channels, LSTM_cells, num_classes):\n",
    "    num_samples, num_segments, num_channels = data.shape\n",
    "    \n",
    "    if num_classes == 4:\n",
    "        Input = tf.keras.Input(shape=(num_segments, num_channels), batch_size=None)\n",
    "        Input_f = tf.keras.layers.Dense(num_reduced_channels)(Input)\n",
    "        Input_f_norm = tf.keras.layers.BatchNormalization()(Input_f)\n",
    "        LSTM = tf.keras.layers.LSTM(LSTM_cells, dropout=0.6)(Input_f_norm)\n",
    "        Output = tf.keras.layers.Dense(num_classes, activation='softmax')(LSTM)\n",
    "\n",
    "        return tf.keras.Model(inputs=Input, outputs=Output)\n",
    "    \n",
    "    else:\n",
    "        Input_K = tf.keras.Input(shape=(num_segments, num_channels), batch_size=None)\n",
    "        Input_A = tf.keras.Input(shape=(num_segments, num_channels), batch_size=None)\n",
    "        Input_K_f = tf.keras.layers.Dense(num_reduced_channels)(Input_K)\n",
    "        Input_A_f = tf.keras.layers.Dense(num_reduced_channels)(Input_A)\n",
    "        Input_K_f_norm = tf.keras.layers.BatchNormalization()(Input_K_f)\n",
    "        Input_A_f_norm = tf.keras.layers.BatchNormalization()(Input_A_f)\n",
    "        LSTM_K = tf.keras.layers.LSTM(LSTM_cells, dropout=0.6)(Input_K_f_norm)\n",
    "        LSTM_A = tf.keras.layers.LSTM(LSTM_cells, dropout=0.6)(Input_A_f_norm)\n",
    "        Merged_LSTM = tf.keras.layers.concatenate([LSTM_K, LSTM_A])\n",
    "        Output = tf.keras.layers.Dense(num_classes, activation='softmax')(Merged_LSTM)\n",
    "\n",
    "        return tf.keras.Model(inputs=[Input_K, Input_A], outputs=Output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a model on a given set of training features and labels\n",
    "_______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _train(K, A, target, spatial_filter_dim, LSTM_cells, num_classes, epochs, learning_rate, verbose, batch_size, log_dir):\n",
    "    \n",
    "    model = _fit(data=K, num_reduced_channels=spatial_filter_dim, LSTM_cells=LSTM_cells, num_classes=num_classes)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
    "    callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    if num_classes == 4:\n",
    "        model.fit([A], target, epochs=epochs, verbose=verbose, batch_size=batch_size, callbacks=[callback])\n",
    "    \n",
    "    else:\n",
    "        model.fit([K, A], target, epochs=epochs, verbose=verbose, batch_size=batch_size, callbacks=[callback])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating a model on a given set of test data and labels\n",
    "_______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(data, target, model, segment_length, num_classes, batch_size, log_dir):\n",
    "    new_target = map_to_one_hot(target)\n",
    "    normalized_data = _preprocessing(data)\n",
    "    segmented_data = _segment_data(normalized_data, segment_length)\n",
    "    K, A = _linear_regression(segmented_data)\n",
    "    callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    if num_classes == 4:\n",
    "        loss, acc = model.evaluate([A], new_target, callbacks=[callback])\n",
    "        pred_label = model.predict(A, batch_size=batch_size)\n",
    "        \n",
    "    else:\n",
    "        loss, acc = model.evaluate([K, A], new_target, callbacks=[callback])\n",
    "        pred_label = model.predict([K, A], batch_size=batch_size)\n",
    "    \n",
    "    return acc, tf.math.confusion_matrix(tf.math.argmax(new_target, axis=1),tf.math.argmax(pred_label, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline for training the model on a given data and labels\n",
    "______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pipeline(data, target, segment_length, spatial_filter_dim, LSTM_cells, num_classes, epochs, batch_size, log_dir, verbose=0, learning_rate=3e-4):\n",
    "    new_target = map_to_one_hot(target)\n",
    "    normalized_data = _preprocessing(data)\n",
    "    segmented_data = _segment_data(normalized_data, segment_length)\n",
    "    K, A = _linear_regression(segmented_data)\n",
    "    model = _train(K, A, target=new_target, \n",
    "                   spatial_filter_dim=spatial_filter_dim, LSTM_cells=LSTM_cells, \n",
    "                   num_classes=num_classes, epochs=epochs, \n",
    "                   learning_rate=learning_rate, verbose=verbose, \n",
    "                   batch_size=batch_size, log_dir=log_dir)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Mapping class labels to a categorical variable.\n",
    " ______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def map_to_one_hot(target):\n",
    "    new_target = np.copy(target)\n",
    "    unique = np.unique(target)\n",
    "    new_labels = np.linspace(0, len(unique)-1, len(unique))\n",
    "    mapping = dict(zip(unique, new_labels))\n",
    "    for idx, label in enumerate(target):\n",
    "        new_target[idx] = mapping[label]\n",
    "    return tf.one_hot(new_target, depth=len(unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a possible training scheme\n",
    "-------\n",
    "Define parameters if interest, i.e. LSTM cells or dimension of the spatial filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_cross_val = 5\n",
    "num_segments = 6\n",
    "t_sec = 3\n",
    "epochs=300\n",
    "spatial_filter_dim, LSTM_cells =  6, 8\n",
    "learning_rate = 3e-4\n",
    "batch_size=64\n",
    "\n",
    "for comb_idx, comb in enumerate([[769, 770], [770, 772]]):\n",
    "    EEG, TARGET, _ = get_npz_data(path=BCI_IV_2a_DATA_PATH, user='A01E', labels=comb, sec=t_sec)\n",
    "    for split_idx, (train_idx, test_idx) in enumerate(StratifiedKFold(n_splits=k_cross_val).split(np.zeros(shape=TARGET.shape), TARGET)):\n",
    "        EEG_TRAIN, TARGET_TRAIN = EEG[train_idx], TARGET[train_idx]\n",
    "        EEG_TEST, TARGET_TEST = EEG[test_idx], TARGET[test_idx]\n",
    "        \n",
    "        num_classes = len(set(TARGET_TRAIN))\n",
    "        _, samples_per_trial, _ = EEG_TRAIN.shape\n",
    "        log_dir = f'{LOG_PATH}/CIDX-{comb_idx}-SIDX-{split_idx}-{comb[0]}-{comb[1]}/'\n",
    "\n",
    "        model = pipeline(data=EEG_TRAIN, target=TARGET_TRAIN, \n",
    "                         segment_length=samples_per_trial//num_segments, \n",
    "                         spatial_filter_dim=spatial_filter_dim, \n",
    "                         LSTM_cells=LSTM_cells, \n",
    "                         num_classes=num_classes,\n",
    "                         learning_rate=learning_rate,\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=epochs,\n",
    "                         log_dir=log_dir)\n",
    "        \n",
    "        acc, _ = evaluate(data=EEG_TEST, target=TARGET_TEST,\n",
    "                          model=model, \n",
    "                          segment_length=samples_per_trial//num_segments,\n",
    "                          num_classes=num_classes,\n",
    "                          batch_size=batch_size,\n",
    "                          log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search\n",
    "----------\n",
    "#### Adapt the parameters _learning_rates_, _filter_dim_, _lstm_cells_, _num_segments_ and _epochs_ before starting the hypterparamter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "GRIDSEARCH_PATH = 'LOG/Gridsearch'\n",
    "\n",
    "learning_rates = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "filter_dim = [3, 6, 9, 12]\n",
    "lstm_cells = [3, 5, 8, 10]\n",
    "num_segments = [3, 5, 6, 10]\n",
    "epochs = [250, 500, 1000]\n",
    "\n",
    "HP_LEARNING_RATE = hp.HParam('Learning Rate', hp.Discrete(learning_rates))\n",
    "HP_FILTER_DIM = hp.HParam('Filter Dimension', hp.Discrete(filter_dim))\n",
    "HP_LSTM_CELLS = hp.HParam('LSTM Cells', hp.Discrete(lstm_cells))\n",
    "HP_NUM_SEGMENTS = hp.HParam('Number of Segments', hp.Discrete(num_segments))\n",
    "HP_EPOCHS = hp.HParam('Epochs', hp.Discrete(epochs))\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "\n",
    "def run(path, hparams, train_data, train_labels, test_data, test_labels, batch_size, samples_per_trial, fold_idx):\n",
    "        hp.hparams(hparams)\n",
    "        model = pipeline(data=train_data, target=train_labels,\n",
    "                         segment_length=samples_per_trial//hparams[HP_NUM_SEGMENTS], \n",
    "                         spatial_filter_dim=hparams[HP_FILTER_DIM], \n",
    "                         LSTM_cells=hparams[HP_LSTM_CELLS], \n",
    "                         num_classes=len(set(train_labels)),\n",
    "                         learning_rate=hparams[HP_LEARNING_RATE],\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=hparams[HP_EPOCHS], \n",
    "                         log_dir=path)\n",
    "        \n",
    "        accuracy, _ = evaluate(data=test_data, target=test_labels,\n",
    "                               model=model,\n",
    "                               segment_length=samples_per_trial//hparams[HP_NUM_SEGMENTS],\n",
    "                               num_classes=len(set(train_labels)),\n",
    "                               batch_size=batch_size,\n",
    "                               log_dir=path)\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG, TARGET, _ = get_npz_data(BCI_IV_2a_DATA_PATH, 'A01E')\n",
    "_, samples_per_trial, _ = EEG.shape\n",
    "session_idx = 0\n",
    "batch_size = 64\n",
    "\n",
    "for lr_idx, lr in enumerate(HP_LEARNING_RATE.domain.values, 0):\n",
    "    for fdim_idx, fdim in enumerate(HP_FILTER_DIM.domain.values, 0):\n",
    "        for cells_idx, cells in enumerate(HP_LSTM_CELLS.domain.values, 0):\n",
    "            for segment_idx, N in enumerate(HP_NUM_SEGMENTS.domain.values, 0):\n",
    "                for epoch_idx, epoch_N in enumerate(HP_EPOCHS.domain.values, 0):\n",
    "                    hparams = {\n",
    "                        HP_LEARNING_RATE: lr,\n",
    "                        HP_FILTER_DIM: fdim,\n",
    "                        HP_LSTM_CELLS: cells,\n",
    "                        HP_NUM_SEGMENTS: N,\n",
    "                        HP_EPOCHS: epoch_N\n",
    "                    }\n",
    "                    \n",
    "                    accuracies = list()\n",
    "                    path = f'{GRIDSEARCH_PATH}/grid-{session_idx}/'\n",
    "                    with tf.summary.create_file_writer(path).as_default(): \n",
    "                        for fold_idx, (train_idx, test_idx) in enumerate(StratifiedKFold(n_splits=5).split(np.zeros(shape=TARGET.shape), TARGET)):\n",
    "                            EEG_TRAIN, TARGET_TRAIN = EEG[train_idx], TARGET[train_idx]\n",
    "                            EEG_TEST, TARGET_TEST = EEG[test_idx], TARGET[test_idx]\n",
    "\n",
    "                            acc = run(path=f'{path}/fold-{fold_idx}', hparams=hparams, \n",
    "                                      train_data=EEG_TRAIN, train_labels=TARGET_TRAIN, \n",
    "                                      test_data=EEG_TEST, test_labels=TARGET_TEST, \n",
    "                                      batch_size=batch_size, samples_per_trial=samples_per_trial,\n",
    "                                      fold_idx=fold_idx)\n",
    "                            accuracies.append(acc)\n",
    "\n",
    "                        tf.summary.scalar(METRIC_ACCURACY, tf.reduce_mean(accuracies), step=session_idx)\n",
    "                        session_idx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
