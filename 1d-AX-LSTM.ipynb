{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.polynomial import Polynomial\n",
    "\n",
    "from utilities import get_npz_data, get_project_data, get_bci_iii_data, get_bci_iii_data\n",
    "\n",
    "tf.random.set_seed(333)\n",
    "logger = logging.getLogger('1d-AX-LSTM')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard\n",
    "------\n",
    "#### Used to track the gridsearch and visualize the process of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir LOG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data acquisition \n",
    "----\n",
    "#### Showing different possibilities of acquiring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LOG_PATH = f'{os.getcwd()}/LOG/GridSearch_1dAX_LSTM'\n",
    "HYBRID_DATA_PATH = f'{os.getcwd()}/Data/Hybrid'\n",
    "BCI_IV_2a_DATA_PATH = f'{os.getcwd()}/Data/BCI'\n",
    "BCI_III_IVa_DATA_PATH = f'{os.getcwd()}/Data/BCI III IVa'\n",
    "\n",
    "L_EEG, L_TARGET = get_project_data(HYBRID_DATA_PATH, [2.0, 3.0], sec=3, offset=1)\n",
    "\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.makedirs(LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "----\n",
    "#### The first preprocessing step is the normalisation. The scheme for this is the follwing. Let us consider $\\mathbf{X}_{\\mathrm{EEG}}=[\\mathbf{x}^{(1)},\\ldots, \\mathbf{x}^{(N)}]\\in \\mathbb{R}^{k \\times N}$, where $k$ are the amount of channels and $N$ the observations. Then we take a vector $\\mathbf{x}_{k}=[\\mathrm{x}^{(1)}_{k},\\ldots, \\mathrm{x}^{(N)}_{k}]$, calculate its mean $\\mu_{k}$, standard deviation $\\sigma_{k}$ and normalise it as $\\mathbf{\\bar{x}}_{k}=[\\frac{\\mathrm{x}^{(1)}_{k}-\\mu_{k}}{\\sigma_{k}},\\ldots, \\frac{\\mathrm{x}^{(N)}_{k}-\\mu_{k}}{\\sigma_{k}}]$. Repeating this for each channel, we get $\\mathbf{\\bar{X}}_{\\mathrm{EEG}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _preprocessing(data):\n",
    "    logger.info(f'Preprocessing data')\n",
    "    normalized_data = np.empty(shape=data.shape)\n",
    "    for idx, trial in enumerate(data):\n",
    "        normalized_data[idx] = StandardScaler().fit_transform(X=trial)\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "### One Dimension-Aggregate Approximation (1d-AX)\n",
    "----\n",
    "#### 1d-AX  can calculated by taking $\\mathrm{\\bar{x}}_{k}=[\\mathrm{\\bar{x}}^{(1)}_{k},\\ldots, \\mathrm{\\bar{x}}^{(N)}_{k}]$ for each channel, and divide it into $m$ segements $\\mathbf{y}_{k}^{(i)}$ of equal length $q=\\frac{N}{m}$. Afterwards, we take a sampled version of $\\mathbf{y}_{k}^{(i)}$ and apply linear regression to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _segment_data(data, seg_length):\n",
    "    logger.info(f'Segmenting data')\n",
    "    num_trials, num_samples, num_channels = data.shape\n",
    "    assert num_samples % seg_length == 0\n",
    "    num_segments = num_samples//seg_length\n",
    "    seg_eeg_data = np.empty(shape=(num_trials, num_segments, seg_length, num_channels))\n",
    "\n",
    "    for trial_idx in range(num_trials):\n",
    "        for segment_idx in range(num_segments):\n",
    "            lower_limit = segment_idx * seg_length\n",
    "            upper_limit = lower_limit + seg_length\n",
    "            seg_eeg_data[trial_idx, segment_idx] = data[trial_idx, lower_limit:upper_limit, :]\n",
    "\n",
    "    return seg_eeg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _linear_regression(data):\n",
    "    num_trials, num_segments, num_samples, num_channels = data.shape\n",
    "    x = np.linspace(1, num_samples, num_samples)\n",
    "\n",
    "    K = np.zeros(shape=(num_trials, num_channels, num_segments))\n",
    "    A = np.zeros(shape=(num_trials, num_channels, num_segments))\n",
    "    for trial_idx, trial in enumerate(data):\n",
    "        for segment_idx, segment in enumerate(trial):\n",
    "            for channel_idx in range(num_channels):\n",
    "                (c1, c0) = np.polyfit(x, segment[:, channel_idx], deg=1)\n",
    "                t_mean = np.mean(segment[:, channel_idx])\n",
    "                K[trial_idx, channel_idx, segment_idx] = c1\n",
    "                A[trial_idx, channel_idx, segment_idx] = c1 * t_mean + c0\n",
    "\n",
    "    return K, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel Weighting\n",
    "----\n",
    "#### Following the 1d-AX step, we calculate spatial filters for dimensionality reduction. The idea is to gain filters $\\mathbf{W_{\\mathbf{K}}}$ and $\\mathbf{W_{\\mathbf{A}}}$, which reduce the dimensionality of $\\mathbf{K}$ and $\\mathbf{A}$ by taking $\\mathbf{K}'=\\mathbf{W_{\\mathbf{K}}}\\mathbf{K}$, as well as $\\mathbf{A}'=\\mathbf{W_{\\mathbf{A}}}\\mathbf{A}$.\n",
    "\n",
    "# LSTM and Softmax Regression\n",
    "----\n",
    "#### Subsequently, the output of the filters $\\mathbf{K}'$, $\\mathbf{A}'$ is fed into two LSTMs in parallel. Output of the LSTM is merged and fed into a Softmax Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _fit(data, num_reduced_channels, LSTM_cells, num_classes):\n",
    "    _, num_segments, num_channels = data.shape\n",
    "\n",
    "    Input_K = tf.keras.Input(shape=(num_segments, num_channels), batch_size=None)\n",
    "    Input_A = tf.keras.Input(shape=(num_segments, num_channels), batch_size=None)\n",
    "    Input_K_f = tf.keras.layers.Dense(num_reduced_channels)(Input_K)\n",
    "    Input_A_f = tf.keras.layers.Dense(num_reduced_channels)(Input_A)\n",
    "    Input_K_f_norm = tf.keras.layers.BatchNormalization()(Input_K_f)\n",
    "    Input_A_f_norm = tf.keras.layers.BatchNormalization()(Input_A_f)\n",
    "    LSTM_K = tf.keras.layers.LSTM(LSTM_cells, dropout=0.6)(Input_K_f_norm)\n",
    "    LSTM_A = tf.keras.layers.LSTM(LSTM_cells, dropout=0.6)(Input_A_f_norm)\n",
    "    Merged_LSTM = tf.keras.layers.concatenate([LSTM_K, LSTM_A])\n",
    "    Output = tf.keras.layers.Dense(num_classes, activation='softmax')(Merged_LSTM)\n",
    "\n",
    "    return tf.keras.Model(inputs=[Input_K, Input_A], outputs=Output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a model on a given set of training features and labels\n",
    "_______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _train(K, A, target, spatial_filter_dim, LSTM_cells, num_classes, epochs, verbose, batch_size):\n",
    "    model = _fit(K, spatial_filter_dim, LSTM_cells, num_classes)\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
    "    model.fit([K, A], target, epochs=epochs, verbose=verbose, batch_size=batch_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating a model on a given set of test data and labels\n",
    "_______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(data, target, model, segment_length, batch_size=64):\n",
    "    new_target = map_to_one_hot(target)\n",
    "    normalized_data = _preprocessing(data)\n",
    "    segmented_data = _segment_data(normalized_data, segment_length)\n",
    "    K, A = _linear_regression(segmented_data)\n",
    "    loss, acc = model.evaluate([K, A], new_target, batch_size=batch_size)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline for training the model on a given data and labels\n",
    "______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pipeline(data, target, segment_length, spatial_filter_dim, LSTM_cells, num_classes, epochs=500, verbose=1, batch_size=64):\n",
    "    new_target = map_to_one_hot(target)\n",
    "    normalized_data = _preprocessing(data)\n",
    "    segmented_data = _segment_data(normalized_data, segment_length)\n",
    "    K, A = _linear_regression(segmented_data)\n",
    "    model = _train(K, A, new_target, spatial_filter_dim, LSTM_cells, num_classes, epochs, verbose, batch_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Mapping class labels to a categorical variable.\n",
    " ______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def map_to_one_hot(target):\n",
    "    new_target = np.copy(target)\n",
    "    unique = np.unique(target)\n",
    "    new_labels = np.linspace(0, len(unique)-1, len(unique))\n",
    "    mapping = dict(zip(unique, new_labels))\n",
    "    for idx, label in enumerate(target):\n",
    "        new_target[idx] = mapping[label]\n",
    "    return tf.one_hot(new_target, depth=len(unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a possible training scheme\n",
    "-------\n",
    "#### Define parameters if interest, i.e. LSTM cells or dimension of the spatial filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_cross_val = 5\n",
    "accs_shifted = list()\n",
    "num_segments = 6\n",
    "t_sec = 3\n",
    "sampling_freq = 256\n",
    "num_samples = t_sec * sampling_freq\n",
    "\n",
    "spatial_filter_dim, LSTM_cells, num_classes =  6, 8, 2\n",
    "\n",
    "for comb in [[0.0, 1.0], [0.0, 2.0], [0.0, 3.0], [1.0, 2.0], [2.0, 3.0], [2.0, 3.0]]:\n",
    "    tmp = list()\n",
    "    e_tmp1, t_tmp1 = get_project_data(HYBRID_DATA_PATH, comb, sec=t_sec, offset=1)\n",
    "    e_tmp2, t_tmp2 = get_project_data(HYBRID_DATA_PATH, comb, sec=t_sec, offset=2)\n",
    "    e_tmp3, t_tmp3 = get_project_data(HYBRID_DATA_PATH, comb, sec=t_sec, offset=3)\n",
    "    EEG, TARGET = np.vstack([e_tmp1, e_tmp2, e_tmp3]), np.hstack([t_tmp1, t_tmp2, t_tmp3])\n",
    "    for train_idx, test_idx in StratifiedKFold(n_splits=k_cross_val).split(np.zeros(shape=TARGET.shape), TARGET):\n",
    "        EEG_TRAIN, TARGET_TRAIN = EEG[train_idx], TARGET[train_idx]\n",
    "        EEG_TEST, TARGET_TEST = EEG[test_idx], TARGET[test_idx]\n",
    "\n",
    "        model = pipeline(EEG_TRAIN, TARGET_TRAIN, num_samples//num_segments, spatial_filter_dim, LSTM_cells, num_classes, )\n",
    "        acc = evaluate(EEG_TEST, TARGET_TEST, model, num_samples//num_segments)\n",
    "        tmp.append(acc)\n",
    "    accs_shifted.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search\n",
    "----------\n",
    "#### Adapt the parameters _learning_rates_, _filter_dim_, _lstm_cells_, _num_segments_ and _epochs_ before starting the hypterparamter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(LOG_PATH)\n",
    "except FileExistsError:\n",
    "    shutil.rmtree(LOG_PATH)\n",
    "finally:\n",
    "    os.makedirs(LOG_PATH)\n",
    "\n",
    "learning_rates = [1e-3, 1e-2, 1e-1]\n",
    "filter_dim = [3, 6, 9, 12]\n",
    "lstm_cells = [1, 3, 5, 8, 10]\n",
    "num_segments = [4, 6, 8]\n",
    "epochs = [250, 500, 1000]\n",
    "\n",
    "HP_LEARNING_RATE = hp.HParam('Learning Rate', hp.Discrete(learning_rates))\n",
    "HP_FILTER_DIM = hp.HParam('Filter Dimension', hp.Discrete(filter_dim))\n",
    "HP_LSTM_CELLS = hp.HParam('LSTM Cells', hp.Discrete(lstm_cells))\n",
    "HP_NUM_SEGMENTS = hp.HParam('Number of Segments', hp.Discrete(num_segments))\n",
    "HP_EPOCHS = hp.HParam('Epochs', hp.Discrete(epochs))\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "def run(path, hparams, train_data, train_labels, test_data, test_labels, fold_idx):\n",
    "    with tf.summary.create_file_writer(path).as_default():\n",
    "        hp.hparams(hparams)\n",
    "        model = pipeline(train_data, train_labels,\n",
    "                         segment_length=768//hparams[HP_NUM_SEGMENTS], \n",
    "                         spatial_filter_dim=hparams[HP_FILTER_DIM], \n",
    "                         LSTM_cells=hparams[HP_LSTM_CELLS], \n",
    "                         num_classes=4,\n",
    "                         epochs=hparams[HP_EPOCHS])\n",
    "        accuracy = evaluate(test_data, test_labels, model, \n",
    "                            segment_length=768//hparams[HP_NUM_SEGMENTS])\n",
    "\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=fold_idx, description=f'Fold {fold_idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_idx = 0\n",
    "for lr_idx, lr in enumerate(HP_LEARNING_RATE.domain.values, 0):\n",
    "    for fdim_idx, fdim in enumerate(HP_FILTER_DIM.domain.values, 0):\n",
    "        for cells_idx, cells in enumerate(HP_LSTM_CELLS.domain.values, 0):\n",
    "            for segment_idx, N in enumerate(HP_NUM_SEGMENTS.domain.values, 0):\n",
    "                for epoch_idx, epoch_N in enumerate(HP_EPOCHS.domain.values, 0):\n",
    "                    hparams = {\n",
    "                        HP_LEARNING_RATE: lr,\n",
    "                        HP_FILTER_DIM: fdim,\n",
    "                        HP_LSTM_CELLS: cells,\n",
    "                        HP_NUM_SEGMENTS: N,\n",
    "                        HP_EPOCHS: epoch_N\n",
    "                    }\n",
    "                    \n",
    "                    EEG, TARGET = get_project_data(HYBRID_DATA_PATH, [0.0, 1.0, 2.0, 3.0], sec=3, offset=1)\n",
    "                    for fold_idx, (train_idx, test_idx) in enumerate(StratifiedKFold(n_splits=5).split(np.zeros(shape=TARGET.shape), TARGET)):\n",
    "                        EEG_TRAIN, TARGET_TRAIN = EEG[train_idx], TARGET[train_idx]\n",
    "                        EEG_TEST, TARGET_TEST = EEG[test_idx], TARGET[test_idx]\n",
    "\n",
    "                        run(f'{LOG_PATH}/t-{session_idx}-fold-{fold_idx}', hparams, EEG_TRAIN, TARGET_TRAIN, EEG_TEST, TARGET_TEST, fold_idx)\n",
    "                        \n",
    "                    session_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
